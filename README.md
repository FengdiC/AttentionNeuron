# Do Attention-based Models Explain Visual Cortex Activity Better Than Non-Attentionâ€“Based Models

This is the repo for the experiment code for CMPUT652 project.

## Abstract
The human visual system has been shown to be influenced by the attention mechanism. In this paper, we hypothesized that the representations obtained from deep neural networks that utilize the attention mechanism correlate higher to the brain activity in the visual cortex as compared to that of the models without the attention mechanism. We test this hypothesis by empirically evaluating the representations obtained from these models and the brain activity fMRI representations through the use of representational similarity analysis and the encoding models.

## RDM plots
![rdm1](/RSA_figs/brain1.png?raw=true "")
![rdm2](/RSA_figs/resnet.png?raw=true "")
![rdm3](/RSA_figs/seresnet.png?raw=true "")
![rdm4](/RSA_figs/skresnext.png?raw=true "")
![rdm5](/RSA_figs/resnest.png?raw=true "")

## Instructions
- Install the requirements using `requirements.txt`. Also obtain the
dataset from the original source here:
<https://github.com/KamitaniLab/GenericObjectDecoding>

- The features can be generated by running the scripts in the
  `feature_generation/` folder.

- For obtaining the RSA results, see `RSA.ipynb`
- For encoding experiments: Generate features by running
  `pure_pca_image.py` and train the models using `encoding.py`.

- For localization experiments, see `3d_brain.py`
